options(scipen = 999)#Do not display exponents
Bike <- read.csv("C:/Users/cweaver/Downloads/Bike Rental UCI dataset.csv")
Bike$weathersit <- as.factor(Bike$weathersit)
Bike$season <- as.factor(Bike$season)
Bike$instant <- NULL
Bike$dteday <- NULL
Bike$casual <- NULL
Bike$registered <- NULL
Bike$yr <- NULL
previous_hrs <- 12
orig_names <- names(Bike)
n_rows <- dim(Bike)[1]
orig_colCnt <- dim(Bike)[2] #number of col in original data after cleaning
suffix <- -1:-previous_hrs #to create new colums
Bike1 <- Bike
for (i in 1:previous_hrs) {
#create new column, start at 2nd row, copy from Col 13 (cnt) - fill in 12 new columns with data from cnt
Bike1[(i+1):n_rows, orig_colCnt+i] <- Bike1[1:(n_rows-i), orig_colCnt]
#Fill in remaining resulting NA with the first cnt record (16)
Bike1[1:i, orig_colCnt+i] <- Bike1[1:i, orig_colCnt+i-1]
}
new_names_hour <- paste("demand in hour", suffix)
names(Bike1) <- c(orig_names, new_names_hour)
Bike2 <- Bike1
orig_colCnt2 <- orig_colCnt + previous_hrs
for (i in 1:previous_hrs) {
Bike2[(i * 24 + 1):n_rows, orig_colCnt2 + i] <- Bike2[1:(n_rows - i * 24), orig_colCnt]
Bike2[1:(i * 24), orig_colCnt2 + i] <- Bike2[1:(i * 24), orig_colCnt2 + i - 1]
}
new_names_day <- paste("demand in day", suffix)
names(Bike2) <- c(orig_names, new_names_hour, new_names_day)
Bike3 <- Bike2
orig_colCnt2 <- orig_colCnt + previous_hrs * 2
for (i in 1:previous_hrs) {
Bike3[(i * 24 * 7 + 1):n_rows, orig_colCnt2 + i] <- Bike3[1:(n_rows - i * 24 * 7), orig_colCnt]
Bike3[1:(i * 24 * 7), orig_colCnt2 + i] <- Bike3[1:(i * 24 * 7), orig_colCnt2 + i - 1]
}
new_names_week <- paste("demand in week", suffix)
names(Bike3) <- c(orig_names, new_names_hour, new_names_day, new_names_week)
simple_tree <- rpart::rpart(cnt~., data = Bike, method = "anova", cp = 0.03)
rpart.plot::rpart.plot(simple_tree)
set.seed <- 12347
trainctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, verboseIter = TRUE)
#Decision Tree
#This version does not like the column names, so do not use the formula method
#rpart_tree <- train(cnt~., data = Bike1, method = "rpart", trControl = trainctrl)
rpart_tree <- train(
x = Bike[, names(Bike) != "cnt"],
y = Bike$cnt,
method = "rpart",
trControl = trainctrl
)
#Random Forest
rf_tree <- train(cnt~., data = Bike1, method = "rf",  trControl = trainctrl)
resamps <- resamples(list(singleTree=rpart_tree, randomForest=rf_tree))
summary(resamps)
setwd("~/Github/Valorem/SalesCollaterals")
save.image("DemandForecast.RData")
summary
resamps
summary(resamps)
gbm_tree <- train(cnt~., data = Bike1, method = "gbm", trControl = trainctrl,
distribution = "gaussian")
gbm_tree
gbm_tree
resamps <- resamples(list(singleTree=rpart_tree, randomForest=rf_tree, GBM = gbm_tree))
summary(resamps)
getModelInfo()$gbm$parameters
myGrid <- expand.grid(n.trees = c(150, 175, 200, 250),
interaction.depth = c(5, 6, 7, 8, 9),
shrinkage = c(0.075, 0.1, 0.125, 0.15, 0.2),
n.minobsinnode = c(7, 10, 12, 15))
gbm_tree2 <- train(cnt~., data = Bike1, method = "gbm", trControl = trainctrl,
distribution = "gaussian", tuneGrid = myGrid)
gbm_tree2
setwd("~/Github/Valorem/SalesCollaterals")
save.image("DemandForecast.RData")
resamps <- resamples(list(singleTree=rpart_tree, randomForest=rf_tree, GBM = gbm_tree,
GBM_Grid = gbm_tree2))
summary(resamps)
gbm_tree2$bestTune
myGrid2 <- gbm_tree2$bestTune
gbm_tree2 <- train(cnt~., data = Bike1, method = "gbm", trControl = trainctrl,
distribution = "gaussian", tuneGrid = myGrid)
myGrid2 <- gbm_tree2$bestTune
gbm_tree2 <- train(cnt~., data = Bike1, method = "gbm", trControl = trainctrl,
distribution = "gaussian", tuneGrid = myGrid2)
resamps <- resamples(list(singleTree=rpart_tree, randomForest=rf_tree, GBM = gbm_tree,
GBM_Grid = gbm_tree2))
summary(resamps)
setwd("~/Github/Valorem/SalesCollaterals")
save.image("DemandForecast.RData")
summary(resamps, metric = "RSME")
dotplot(resamps, metric = "RSME", main = "Model Compare")
dotplot(resamps, metric = "RMSE", main = "Model Compare")
boxplot(resamps, metric = "RMSE", main = "Model Compare")
densityplot(resamps, metric = "RMSE", main = "Model Compare")
bwplot(resamps, metric = "RMSE", main = "Model Compare")
splom(resamps, metric = "RMSE", main = "Model Compare")
bwplot(resamps, metric = "RMSE", main = "Model Compare")
varImp(gbm_tree2)
varImp(gbm_tree)
varImp(gbm_tree)
caret::varImp(gbm_tree)
summary.gbm(gbm_tree)
??varImp
varImp(gbm_tree, 250)
gbm_tree2 <- train(cnt~., data = Bike1, method = "gbm", trControl = trainctrl,
distribution = "gaussian", tuneGrid = myGrid2, importance = TRUE)
varImp(gbm_tree2$finalModel)
varImp(gbm_tree2$finalModel, n.trees = 250)
varImp(gbm_tree2$finalModel, scale = FALSE)
varImp(gbm_tree2, scale = FALSE)
??relative.influence
library(gbm)
varImp(gbm_tree2, scale = FALSE)
myGrid2 <- gbm_tree2$bestTune
gbm_tree2 <- train(cnt~., data = Bike1, method = "gbm", trControl = trainctrl,
distribution = "gaussian", tuneGrid = myGrid2)
gbm::varImp(gbm_tree2)
gbm::varImp(gbm_tree2, scale = FALSE)
varImp(gbm_tree2, scale = FALSE)
summary.gbm(gbm_tree2, plotit=TRUE)
summary.gbm(gbm_tree1, plotit=TRUE)
summary.gbm(gbm_tree, plotit=TRUE)
plot(varImp(gbm_tree2, scale = FALSE))
plot(varImp(gbm_tree2, scale = TRUE))
plot(varImp(gbm_tree, scale = TRUE))
library(xgboost)
library(Matrix)
str(Bike1)
myIndex <- sample(2, nrow(Bike1), replace = TRUE,  prob = c(.8, .2))
myIndex <- sample(2, nrow(Bike1), replace = TRUE,  prob = c(.8, .2))
myTrain <-  Bike1[myIndex == 1,]
myTest <- Bike1[myIndex == 2,]
glimpse(Bike1)
names(Bike1)
#Recall there are 2 factors - use one hot encoding & create matrix
# cnt is in the 12th column (names(Bike1))
myTrain_matrix <- sparse.model.matrix(cnt ~.-12, data = myTrain)
#Recall there are 2 factors - use one hot encoding & create matrix
# cnt is in the 12th column (names(Bike1))
myTrain_matrix <- sparse.model.matrix(cnt ~ .-12, data = myTrain)
myTrain
#Recall there are 2 factors - use one hot encoding & create matrix
# cnt is in the 12th column (names(Bike1))
myTrain_matrix <- sparse.model.matrix(cnt ~ .-12, data = myTrain)
glimpse(Bike1)
#First need to remove spaces in column names - xgboost does not like these!
names(myTrain)<-gsub("\\s", "_", names(myTrain))
glimpse(Bike1)
myTrain_matrix <- sparse.model.matrix(cnt ~ .-12, data = myTrain)
names(myTrain)
names(myTrain) <- gsub("-", "_", names(myTrain))
names(myTrain)
myTrain_matrix <- sparse.model.matrix(cnt ~ .-12, data = myTrain)
myTrain_matrix <- sparse.model.matrix(cnt ~ -12, data = myTrain)
myTrain_matrix <- sparse.model.matrix(cnt ~. -12, data = myTrain)
myTrain_matrix <- sparse.model.matrix(cnt ~. -cnt, data = myTrain)
myTrain <-  Bike1[myIndex == 1,]
myTrain_matrix <- sparse.model.matrix(cnt ~. -cnt, data = myTrain)
myTrain <-  Bike1[myIndex == 1,]
#First need to remove spaces in column names - xgboost does not like these!
names(myTrain) <- gsub("\\s", "_", names(myTrain))
myTrain_matrix <- sparse.model.matrix(cnt ~. -cnt, data = myTrain)
names(myTrain) <- gsub("-", "_", names(myTrain))
myTrain_matrix <- sparse.model.matrix(cnt ~. -cnt, data = myTrain)
glimpse(myTrain_matrix)
head(myTrain_matrix)
myTrain_label <-  myTrain[, "cnt"]
myTrain_m <- sparse.model.matrix(cnt ~. -cnt, data = myTrain)
myTrain_matrix <- xgb.DMatrix(data = as.matrix(myTrain_m), label = myTrain_label)
names(myTest) <- gsub("\\s", "_", names(myTest))
names(myTest) <- gsub("-", "_", names(myTest))
names(myTest) <- gsub("\\s", "_", names(myTest))
names(myTest) <- gsub("-", "_", names(myTest))
#Do same thing for Test data
myTest_m <- sparse.model.matrix(cnt ~. -cnt, data = myTest)
myTest_label <-  myTest[, "cnt"]
myTest_matrix <- xgb.DMatrix(data = as.matrix(myTest_m), label = myTest_label)
#Parameters
xgb_params <- list("objective" = "reg:linear", "eval_metric" = "rmse")
watchlist <- list(train = myTrain_matrix, test = myTest_matrix)
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 100,
watchlist = watchlist)
best_model
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
plot(myErrors)
plot(myErrors$iter, myErrors$train_rmse, myErrors$test_rmse)
plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
min(myErrors$test_rmse)
myErrors[myErrors$test_rmse == min(myErrors$test_rmse),]
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 100,
watchlist = watchlist,
eta = 0.1)
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 100,
watchlist = watchlist,
eta = 0.08)
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
myImportance <- xgb.importance(colnames(myTrain_matrix), model = best_model)
print(myImportance)
xgb.plot.importance(myImportance)
myPredictions <- predict(best_model, newdata = myTest_matrix)
head(myPredictions)
myConfusionMatrix <- matrix(myPredictions %>% t() %>% data.frame())
myConfusionMatrix
myConfusionMatrix <- matrix(myPredictions %>% t() %>% data.frame()) %>%
mutate(label = myTest_label)
myTest_label
myPredictions
myConfusionMatrix <- matrix(myPredictions %>% t() %>% data.frame()) %>%
mutate(label = myTest_label, Prediction = myPredictions)
??featurePlot
myCompare <- data.frame(Prediction = myPredictions, Actual = myTest_matrix)
myCompare <- data.frame(Prediction = myPredictions, Actual = data.frame(myTest_matrix))
myCompare <- data.frame(Prediction = myPredictions, Actual = myTest$cnt)
head(myCompare)
#http://xgboost.readthedocs.io/en/latest/parameter.html
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 100,
watchlist = watchlist,
eta = 0.08,
max.depth = 10,
gamma = 5,
subsample = .8,
colsample_bytree = .9,
missing = NA, seed = 12345)
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
#http://xgboost.readthedocs.io/en/latest/parameter.html
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 100,
watchlist = watchlist,
eta = 0.03,
max.depth = 10,
gamma = 5,
subsample = .8,
colsample_bytree = .9,
missing = NA, seed = 12345)
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
#http://xgboost.readthedocs.io/en/latest/parameter.html
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 100,
watchlist = watchlist,
eta = 0.02,
max.depth = 10,
gamma = 5,
subsample = .8,
colsample_bytree = .9,
missing = NA, seed = 12345)
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
#http://xgboost.readthedocs.io/en/latest/parameter.html
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 300,
watchlist = watchlist,
eta = 0.02,
max.depth = 10,
gamma = 5,
subsample = .8,
colsample_bytree = .9,
missing = NA, seed = 12345)
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
#http://xgboost.readthedocs.io/en/latest/parameter.html
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 300,
watchlist = watchlist,
eta = 0.02,
max.depth = 15,
gamma = 5,
subsample = .8,
colsample_bytree = .9,
missing = NA, seed = 12345)
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
#http://xgboost.readthedocs.io/en/latest/parameter.html
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 300,
watchlist = watchlist,
eta = 0.02,
max.depth = 15,
gamma = 50,
subsample = .8,
colsample_bytree = .9,
missing = NA, seed = 12345)
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
#http://xgboost.readthedocs.io/en/latest/parameter.html
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 300,
watchlist = watchlist,
eta = 0.02,
max.depth = 15,
gamma = 50,
subsample = .5,
colsample_bytree = .9,
missing = NA, seed = 12345)
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
#http://xgboost.readthedocs.io/en/latest/parameter.html
best_model <- xgb.train(params = xgb_params, data = myTrain_matrix, nrounds = 300,
watchlist = watchlist,
eta = 0.02,
max.depth = 5,
gamma = 50,
subsample = .5,
colsample_bytree = .9,
missing = NA, seed = 12345)
#Plot results
myErrors <- data.frame(best_model$evaluation_log)
{plot(myErrors$iter, myErrors$train_rmse, col = "blue")
lines(myErrors$iter, myErrors$test_rmse, col = "red")}
myPredictions <- predict(best_model, newdata = myTest_matrix)
myCompare <- data.frame(Prediction = myPredictions, Actual = myTest$cnt)
head(myCompare, 20)
data(schedulingData)
??schedulingData
library(AppliedPredictiveModeling)
data(schedulingData)
str(schedulingData)
library(AppliedPredictiveModeling)
data(schedulingData)
str(schedulingData)
library(AppliedPredictiveModeling)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
data(schedulingData)
View(schedulingData)
Sys.getenv("PATH")
version()
ver()
which R
R.version()
R.version
titanic_train_orig <- read.csv("~/Github/bananas/data/titanic_train_orig.csv")
View(titanic_train_orig)
titanic_modified <- titanic_train_orig
View(titanic_modified)
table(titanic_modified)
sum(is.na(titanic_modified))
sum(is.null(titanic_modified))
table(titanic_modified$Cabin)
table(titanic_modified$Sex)
titanic_modified[13, 5] <-  NULL
titanic_modified[13, 5]
titanic_modified[13, 5] <-  NULL
gtools::invalid(titanic_modified$Age)
gtools::invalid(titanic_modified$Age, NA)
gtools::invalid(titanic_modified$Age, NA)
gtools::invalid(titanic_modified$Age)
gtools::invalid(titanic_modified$Cabin)
isnothing = function(x) {
any(is.null(x))  || any(is.na(x))  || any(is.nan(x))
}
isnothing(titanic_modified$Age)
titanic_modified[340:345, "Pclass"] <- NULL
titanic_modified[, NullCol] <- NULL
titanic_modified$NullCol <- NULL
Sys.getenv('PATH')
#RTools 64 bit required first!
library(devtools)
options(devtools.install.args = "--no-multiarch") # if you have 64-bit R only, you can skip this
install_github("Microsoft/LightGBM", subdir = "R-package")
library(doMC)
registerDoMC(cores = 4)
install.packages("doMC")
prop <- ff::read.csv.ffdf(file.choose())
prop <- ff::read.csv.ffdf(file=file.choose())
prop <- fread(file.choose(), header = TRUE)
??fread
o
prop <- data.tabe::fread(file.choose(), header = TRUE)
prop <- data.table::fread(file.choose(), header = TRUE)
# Read in the data
train <- data.table::fread(file.choose())
# Join train and properties
df_train <- train %>% left_join(prop, by = 'parcelid')
library(dplyr)
# Join train and properties
df_train <- train %>% left_join(prop, by = 'parcelid')
rm(train)
x_train <- df_train %>% select(-c(parcelid, logerror, transactiondate, propertyzoningdesc,
propertycountylandusecode))
y_train <- df_train$logerror
rm(df_train)
train_columns <- colnames(x_train)
# Get test and validate groups
set.seed(3828)
xvs = rbinom(nrow(x_train), 1, .05)
x_valid <- x_train[xvs==1,]; y_valid <- y_train[xvs==1]
x_train <- x_train[xvs==0, ]; y_train <- y_train[xvs==0]
# lightgbm params
params <- list(max_bin = 9,
learning_rate = 0.0021,
boosting_type = 'gbdt',
objective = "regression",
metric = 'mae',
sub_feature = 0.5,
bagging_fraction = 0.85,
bagging_freq = 20,
num_leaves = 60,
min_data = 500,
min_hessian = 0.05)
x_train[] <- lapply(x_train, as.numeric)
x_valid[]<-lapply(x_valid, as.numeric)
dtrain <- lgb.Dataset(as.matrix(x_train), label = y_train)
library(lightgbm)
dtrain <- lgb.Dataset(as.matrix(x_train), label = y_train)
dvalid <- lgb.Dataset(as.matrix(x_valid), label = y_valid)
valids <- list(test = dvalid)
clf = lightgbm(params = params, dtrain, nrounds = 500, valids, early_stopping_rounds=40) # I used 500 for nrounds
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("plyr", "dplyr", "ggplot2", "readr", "funModeling", prompt = FALSE)
Users <- read_csv("C:/Users/cweaver/Downloads/Users.csv")
Users <- read_csv("C:/Users/cweaver/Downloads/Users.csv", progress = FALSE)
dim(Users)
Users <- sample_n(Users, 5000)
Users <- sample_n(Users, 5000)
glimpse(Users)
plot_num(Users)
plot_num(Users)
plot_num(Users)
freq(Users)
freq(Users, input = c("Gender", "UserType"))
if(!require(easypackages)){install.packages("easypackages")}
library(easypackages)
packages("plyr", "dplyr", "ggplot2", "readr", "funModeling", prompt = FALSE)
Users <- read_csv("C:/Users/cweaver/Downloads/Users.csv", progress = FALSE)
dim(Users)
print("There are 1.75 M records and 48 features in the User Data.")
Users <- sample_n(Users, 5000)
glimpse(Users)
plot_num(Users, -UserId)
plot_num(select(Users, -UserId)
plot_num(select(Users, -UserId))
tmp <- c("i", "love", "you")
tmp
setwd("~/Github/bananas")
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
if(!require(easypackages)){install.packages("easypackages")}
packages("Hmisc", "funModeling", "GGally", "VIM", "DataExplorer", "PerformanceAnalytics",
"corrplot", "dplyr", "tidyr", "knitr", "mice","kableExtra",  prompt = FALSE)
dataset <- read_delim(file.choose() , delim = ",", col_names = TRUE)
options(warn=-1)
if(!require(easypackages)){install.packages("easypackages")}
packages("Hmisc", "funModeling", "GGally", "VIM", "DataExplorer", "PerformanceAnalytics",
"corrplot","plyr", "dplyr", "tidyr", "knitr", "mice","kableExtra", "readr", prompt = FALSE)
dataset <- read_delim(file.choose() , delim = ",", col_names = TRUE)
## Describe dataset dimensions
dim(dataset)
## Examine dataset
df_status(dataset)
## Parse column name to look for ID to suggest ID columns
filter(df_status(dataset), grepl("id", variable, ignore.case = TRUE)) %>% select(variable, type, unique)
## Parse column name to look for ID to suggest ID columns
myIDs <- filter(df_status(dataset), grepl("id", variable, ignore.case = TRUE)) %>% select(variable, type, unique)
myIDs[,1]
length(myIDs)
View(myIDs)
myIDs[,1]
nrow(myIDs)
if(nrow(myIDs) > 0){
for(i in 1:nrow(myIDs)){
dataset <- dataset %>% select(-myIDs[i,1])
}
}
if(nrow(myIDs) > 0){dataset <- dataset %>% select(-c(myIDs[i,]))}
myIDList <- list(myIDs[,1])
myIDList
if(nrow(myIDs) > 0){dataset <- dataset %>% select(-c(myIDList)}
if(nrow(myIDs) > 0){
myIDList <- list(myIDs[,1])
dataset <- dataset %>% select(-c(myIDList))}
unlist(myIDList)
if(nrow(myIDs) > 0){
dataset <- dataset %>% select(!contains("id"))
}
if(nrow(myIDs) > 0){
dataset <- dataset %>% select(-contains("id"))
}
View(dataset)
#dplyr statement, take nulls and make them NA. ALSO add a flag column that designates that the row had null
dataset  <-  dataset %>% replace(.=="NULL", NA)
df_status((dataset))
library(caret)
nzv <- nearZeroVar(dataset, saveMetrics = TRUE)
nzv
## List columns rank ordered by variability
rowcount <- nrow(dataset)
df_status(dataset) %>% select(variable, unique) %>% mutate(rowcount = rowcount, Distinct = rowcount/unique) %>% arrange(desc(unique))
